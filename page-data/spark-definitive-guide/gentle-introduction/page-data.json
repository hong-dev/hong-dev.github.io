{"componentChunkName":"component---src-templates-blog-post-js","path":"/spark-definitive-guide/gentle-introduction/","result":{"data":{"site":{"siteMetadata":{"title":"hong_devlog","author":"hongdev","siteUrl":"https://hong-dev.github.io","comment":{"disqusShortName":"","utterances":"hong-dev/hong-dev.github.io"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"id":"acbbf733-75b6-5f0e-8ed1-a30ce1fd2a80","excerpt":"Spark: The Definitive Guide 내용 정리 Cluster 여러 컴퓨터의 자원을 모아 하나의 컴퓨터처럼 사용할 수 있게 만든다. Cluster에서 작업을 조율할 수 있는 framework가 필요한데, spark가 그런 역할을 하는 framework Spark Application Spark는 사용 가능한 자원을 파악하기 위해 cluster manager(ex. spark standalone cluster manager, hadoop YARN, Mesos)를 사용 Driver…","html":"<blockquote>\n<p><em>Spark: The Definitive Guide 내용 정리</em></p>\n</blockquote>\n<br>\n<h1 id=\"cluster\" style=\"position:relative;\"><a href=\"#cluster\" aria-label=\"cluster permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cluster</h1>\n<ul>\n<li>여러 컴퓨터의 자원을 모아 하나의 컴퓨터처럼 사용할 수 있게 만든다.</li>\n<li>Cluster에서 작업을 조율할 수 있는 framework가 필요한데, spark가 그런 역할을 하는 framework</li>\n</ul>\n<br>\n<br>\n<h1 id=\"spark-application\" style=\"position:relative;\"><a href=\"#spark-application\" aria-label=\"spark application permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Application</h1>\n<ul>\n<li>Spark는 사용 가능한 자원을 파악하기 위해 cluster manager(ex. spark standalone cluster manager, hadoop YARN, Mesos)를 사용</li>\n</ul>\n<h2 id=\"driver-process\" style=\"position:relative;\"><a href=\"#driver-process\" aria-label=\"driver process permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Driver process</h2>\n<ul>\n<li>SparkSession</li>\n<li>Cluster node 중 하나에서 실행된다.</li>\n<li><code class=\"language-text\">main()</code> 함수를 실행</li>\n<li>Spark application 정보 관리, 입력에 대한 응답, 전반적인 executor process의 작업과 관련된 분석, 배포, scheduling 역할 수행</li>\n<li>Application 수명 주기 동안 관련 정보를 모두 유지</li>\n<li>Spark의 언어 API를 통해 다양한 언어(scala, java, python, SQL, R)로 실행 가능</li>\n</ul>\n<h2 id=\"executor-process\" style=\"position:relative;\"><a href=\"#executor-process\" aria-label=\"executor process permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Executor process</h2>\n<ul>\n<li>Driver process가 할당한 코드를 실행하고, 진행 상황을 driver node에 보고</li>\n<li>각 node에 할당할 executor 수 지정 가능</li>\n<li>대부분 spark code를 실행하는 역할</li>\n</ul>\n<br>\n<br>\n<h1 id=\"sparksession\" style=\"position:relative;\"><a href=\"#sparksession\" aria-label=\"sparksession permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SparkSession</h1>\n<ul>\n<li>SparkSession이라 불리는 driver process로 spark application을 제어한다.</li>\n<li>SparkSession instance는 사용자의 처리 명령을 cluster에서 실행</li>\n<li>하나의 SparkSession은 하나의 Spark application에 대응한다.</li>\n<li>\n<p>대화형 모드(python의 경우 command: <code class=\"language-text\">pyspark</code>)로 spark를 시작하면 SparkSession이 자동으로 생성된다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">spark\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token operator\">&lt;</span>pyspark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>session<span class=\"token punctuation\">.</span>SparkSession <span class=\"token builtin\">object</span> at <span class=\"token number\">0x7fe4d2281280</span><span class=\"token operator\">></span></code></pre></div>\n</li>\n</ul>\n<br>\n<h3 id=\"example-spark-dataframe-생성\" style=\"position:relative;\"><a href=\"#example-spark-dataframe-%EC%83%9D%EC%84%B1\" aria-label=\"example spark dataframe 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Example: Spark DataFrame 생성</h3>\n<ul>\n<li>Cluster mode에서 아래의 예제를 실행하면, 숫자 범위의 각 부분이 서로 다른 executor에 할당된다.</li>\n<li>\n<p>이 숫자들은 <strong>distributed collection</strong>이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">my_range <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"number\"</span><span class=\"token punctuation\">)</span>\n\nmy_range\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> DataFrame<span class=\"token punctuation\">[</span>number<span class=\"token punctuation\">:</span> bigint<span class=\"token punctuation\">]</span>\n\nmy_range<span class=\"token punctuation\">.</span>toPandas<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span>\n     number\n<span class=\"token number\">0</span>         <span class=\"token number\">0</span>\n<span class=\"token number\">1</span>         <span class=\"token number\">1</span>\n<span class=\"token number\">2</span>         <span class=\"token number\">2</span>\n<span class=\"token number\">3</span>         <span class=\"token number\">3</span>\n<span class=\"token number\">4</span>         <span class=\"token number\">4</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>      <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n<span class=\"token number\">995</span>     <span class=\"token number\">995</span>\n<span class=\"token number\">996</span>     <span class=\"token number\">996</span>\n<span class=\"token number\">997</span>     <span class=\"token number\">997</span>\n<span class=\"token number\">998</span>     <span class=\"token number\">998</span>\n<span class=\"token number\">999</span>     <span class=\"token number\">999</span>\n<span class=\"token punctuation\">[</span><span class=\"token number\">1000</span> rows x <span class=\"token number\">1</span> columns<span class=\"token punctuation\">]</span></code></pre></div>\n</li>\n</ul>\n<br>\n<br>\n<h1 id=\"dataframe\" style=\"position:relative;\"><a href=\"#dataframe\" aria-label=\"dataframe permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataFrame</h1>\n<ul>\n<li>가장 대표적인 <strong>Structured API</strong></li>\n<li>Table의 data를 row와 column으로 단순하게 표현</li>\n<li><strong>Schema:</strong> column과 column type을 정의한 목록</li>\n<li>Spark DataFrame은 데이터센터의 수천 대의 컴퓨터에 분산되어 있다.</li>\n<li>Python과 R의 DataFrame은 일반적으로 분산 컴퓨터가 아닌 단일 컴퓨터에 존재</li>\n<li>Spark는 Python 과 R 언어를 지원하기 때문에, Python Pandas library의 DataFrame과 R의 DataFrame을 Spark DataFrame으로 쉽게 변환 가능</li>\n</ul>\n<br>\n<h1 id=\"partition\" style=\"position:relative;\"><a href=\"#partition\" aria-label=\"partition permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Partition</h1>\n<ul>\n<li>Cluster의 물리적 machine에 존재하는 row의 집합으로, 실행 중에 data가 computer cluster에서 물리적으로 분산되는 방식을 나타낸다.</li>\n<li>모든 executor가 병렬로 작업을 수행할 수 있도록 partition이라 불리는 chunk 단위로 데이터를 분할</li>\n<li>Partition이 하나이면 executor가 많아도 병렬성은 1이 되고, partition이 많고 executor가 하나인 경우에도 병렬성은 1이 된다.</li>\n<li>DataFrame을 사용하면, partition을 수동 혹은 개별적으로 처리할 필요가 없다.</li>\n</ul>\n<br>\n<br>\n<h1 id=\"transformation\" style=\"position:relative;\"><a href=\"#transformation\" aria-label=\"transformation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transformation</h1>\n<ul>\n<li>Spark의 핵심 데이터 구조는 immutable 하다.</li>\n<li>DataFrame을 변경하려면 원하는 방법을 spark에 알려줘야 하는데, 이 때 사용하는 명령을 transformation이라고 한다.</li>\n</ul>\n<br>\n<h3 id=\"example-find-even-number\" style=\"position:relative;\"><a href=\"#example-find-even-number\" aria-label=\"example find even number permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Example: Find even number</h3>\n<ul>\n<li>\n<p>아래의 코드를 실행해도 결과는 출력되지 않는다.</p>\n<p>추상적인 transformation만 지정한 상태이기 때문에, action을 호출하지 않으면 실제 transformation을 수행하지 않는다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">even_num <span class=\"token operator\">=</span> my_range<span class=\"token punctuation\">.</span>where<span class=\"token punctuation\">(</span><span class=\"token string\">\"number % 2 = 0\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h3 id=\"narrow-dependency\" style=\"position:relative;\"><a href=\"#narrow-dependency\" aria-label=\"narrow dependency permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Narrow dependency</h3>\n<ul>\n<li>좁은 의존성을 가진 transformation은 각 입력 partition이 하나의 출력 partition에만 영향을 미친다. (1:1)</li>\n<li>\n<p><strong>Pipelining</strong>이 자동으로 수행된다.</p>\n<p>즉, DataFrame에 여러 필터를 지정하는 경우 모든 작업이 메모리에서 일어난다.</p>\n</li>\n<li>위의 where 구문은 좁은 의존성을 가진다.</li>\n</ul>\n<h3 id=\"wide-dependency\" style=\"position:relative;\"><a href=\"#wide-dependency\" aria-label=\"wide dependency permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Wide dependency</h3>\n<ul>\n<li>넓은 의존성을 가진 transformation은 하나의 입력 partition이 여러 출력 partition에 영향을 미친다. (1:N)</li>\n<li><strong>Shuffle:</strong> 스파크가 cluster에서 partition을 교환하는 것</li>\n<li>스파크는 shuffle의 결과를 disk에 저장한다.</li>\n</ul>\n<br>\n<br>\n<h1 id=\"lazy-evaluation\" style=\"position:relative;\"><a href=\"#lazy-evaluation\" aria-label=\"lazy evaluation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lazy Evaluation</h1>\n<ul>\n<li>Spark가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식</li>\n<li>실제 코드를 실행하기 전에 그 로직을 기본 실행 계획으로 compile 한다.</li>\n<li>\n<p>Example: DataFrame의 <strong>Predicate Pushdown</strong>(조건절 푸시다운)</p>\n<ul>\n<li>\n<p>복잡한 spark job이 원시 data에서 하나의 row만 가져오는 filter를 가지고 있다면, 필요한 record 하나만 읽는 것이 가장 효율적이다.</p>\n<p>spark는 이 filter를 data source로 위임하는 최적화 작업을 자동으로 수행한다.</p>\n<p>예를 들어 데이터 저장소가 database라면 where 절의 처리를 database에 위임하고, spark는 하나의 record만 받는다.</p>\n</li>\n</ul>\n</li>\n</ul>\n<br>\n<h1 id=\"action\" style=\"position:relative;\"><a href=\"#action\" aria-label=\"action permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Action</h1>\n<ul>\n<li>Transformation으로부터 결과를 실제로 계산하도록 지시하는 명령</li>\n<li>Action을 지정하면 Spark <strong>Job</strong>이 시작된다.</li>\n<li><strong>Spark Job</strong>은 개별 action에 의해 trigger되는 다수의 transformation으로 이루어져있다.</li>\n</ul>\n<br>\n<h3 id=\"example-return-the-number-of-records\" style=\"position:relative;\"><a href=\"#example-return-the-number-of-records\" aria-label=\"example return the number of records permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Example: Return the number of records</h3>\n<ul>\n<li>\n<p>아래의 예시에서 spark Job은 filter (narrow transformation)를 수행한 후 partition 별로 record 수를 count (wide transformation) 한다.</p>\n<p>그리고 각 언어에 적합한 native 객체에 결과를 모은다.</p>\n<p>이 때 spark가 제공하는 spark UI로 cluster에서 실행 중인 spark job을 monitoring 할 수 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">even_num<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token number\">500</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h1 id=\"spark-ui\" style=\"position:relative;\"><a href=\"#spark-ui\" aria-label=\"spark ui permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark UI</h1>\n<ul>\n<li>Spark Job의 진행 상황을 monitoring 할 때 사용</li>\n<li>Spark Job의 상태, 환경 설정, cluster 상태 등 확인 가능</li>\n<li>Spark Job을 tuning 하고 debugging 할 때 매우 유용</li>\n<li>Driver node의 4040 port로 접속 가능 (ex. <code class=\"language-text\">http://localhost:4040</code>)</li>\n</ul>\n<br>\n<br>\n<h1 id=\"examples\" style=\"position:relative;\"><a href=\"#examples\" aria-label=\"examples permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Examples</h1>\n<h3 id=\"read\" style=\"position:relative;\"><a href=\"#read\" aria-label=\"read permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>read</h3>\n<ul>\n<li><strong>SparkSession</strong>의 <strong>DataFrameReader</strong> class를 사용해서 데이터를 읽는다.</li>\n<li>DataFrame은 불특정 다수의 row와 column을 가지는데, data를 읽는 과정이 lazy 연산 형태의 transformation이기 때문에 row 수를 알 수 없다.</li>\n<li>\n<p><strong><code class=\"language-text\">inferSchema</code>:</strong> DataFrame의 schema 정보를 알아내는 <strong>schema inference</strong> 기능 사용 가능</p>\n<p>각 column의 data type을 추론하기 위해 적은 양의 data를 읽는다.</p>\n</li>\n<li><strong><code class=\"language-text\">header</code>:</strong> true인 경우 파일의 첫 row를 header로 지정</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">data <span class=\"token operator\">=</span> spark\\\n  <span class=\"token punctuation\">.</span>read\\\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"inferSchema\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"true\"</span><span class=\"token punctuation\">)</span>\\\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"header\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"true\"</span><span class=\"token punctuation\">)</span>\\\n  <span class=\"token punctuation\">.</span>csv<span class=\"token punctuation\">(</span><span class=\"token string\">\"/data/summary.csv\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<h3 id=\"take\" style=\"position:relative;\"><a href=\"#take\" aria-label=\"take permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>take</h3>\n<ul>\n<li>DataFrame의 <code class=\"language-text\">take</code> action을 호출하면, <code class=\"language-text\">head</code> 명령과 같은 결과를 확인할 수 있다.</li>\n<li>csv file⇒ <code class=\"language-text\">read</code> ⇒ DataFrame ⇒ <code class=\"language-text\">take(n)</code> ⇒ Array(Row(…), Row(…))</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">data<span class=\"token punctuation\">.</span>take<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span>\n<span class=\"token punctuation\">[</span>Row<span class=\"token punctuation\">(</span>DEST_COUNTRY_NAME<span class=\"token operator\">=</span><span class=\"token string\">u'United States'</span><span class=\"token punctuation\">,</span> ORIGIN_COUNTRY_NAME<span class=\"token operator\">=</span><span class=\"token string\">u'Romania'</span><span class=\"token punctuation\">,</span> count<span class=\"token operator\">=</span><span class=\"token number\">15</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\nRow<span class=\"token punctuation\">(</span>DEST_COUNTRY_NAME<span class=\"token operator\">=</span><span class=\"token string\">u'United States'</span><span class=\"token punctuation\">,</span> ORIGIN_COUNTRY_NAME<span class=\"token operator\">=</span><span class=\"token string\">u'Ireland'</span><span class=\"token punctuation\">,</span> count<span class=\"token operator\">=</span><span class=\"token number\">344</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n<br>\n<h3 id=\"sort\" style=\"position:relative;\"><a href=\"#sort\" aria-label=\"sort permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>sort</h3>\n<ul>\n<li><code class=\"language-text\">sort</code> method는 DataFrame을 변경하지 않는다.</li>\n<li>Transformation으로 <code class=\"language-text\">sort</code> method를 사용하면, 이전 DataFrame을 변환한 새로운 DataFrame을 생성해 return 한다.</li>\n<li>단지 transformation이기 때문에 호출 시 data에 아무런 변화도 일어나지 않지만, spark는 실행 계획을 만들고 검토하여 cluster에서 처리할 방법을 알아낸다.</li>\n<li>csv file ⇒ <code class=\"language-text\">read (narrow transformation)</code> ⇒ DataFrame ⇒ <code class=\"language-text\">sort (wide transformation)</code> ⇒ DataFrame ⇒ <code class=\"language-text\">take(n)</code> ⇒ Array(…)</li>\n</ul>\n<br>\n<h3 id=\"explain\" style=\"position:relative;\"><a href=\"#explain\" aria-label=\"explain permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>explain</h3>\n<ul>\n<li>DataFrame의 계보(lineage)나 spark의 query 실행 계획 확인 가능</li>\n<li>출력된 실행 계획에서 최종 결과는 가장 위에, data source는 가장 아래에 있다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">data<span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span><span class=\"token string\">\"count\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>explain<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span>\n<span class=\"token operator\">==</span> Physical Plan <span class=\"token operator\">==</span>\n<span class=\"token operator\">*</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span> Sort <span class=\"token punctuation\">[</span>count<span class=\"token comment\">#130 ASC NULLS FIRST], true, 0</span>\n<span class=\"token operator\">+</span><span class=\"token operator\">-</span> Exchange rangepartitioning<span class=\"token punctuation\">(</span>count<span class=\"token comment\">#130 ASC NULLS FIRST, 200)   # 200은 partition 개수</span>\n   <span class=\"token operator\">+</span><span class=\"token operator\">-</span> <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> FileScan csv <span class=\"token punctuation\">[</span>DEST_COUNTRY_NAME<span class=\"token comment\">#128,ORIGIN_COUNTRY_NAME#129,count#130] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int></span></code></pre></div>\n<br>\n<h3 id=\"shufflepartitions\" style=\"position:relative;\"><a href=\"#shufflepartitions\" aria-label=\"shufflepartitions permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>shuffle.partitions</h3>\n<ul>\n<li>Spark는 shuffle 수행 시 기본적으로 200개의 shuffle partition을 생성</li>\n<li>이 값을 변경하면 runtime이 크게 달라질 수 있다.</li>\n<li>csv file ⇒ <code class=\"language-text\">read (narrow transformation)</code> ⇒ DataFrame (1 partition) ⇒ <code class=\"language-text\">sort (wide transformation)</code> ⇒ DataFrame (5 partitions) ⇒ <code class=\"language-text\">take(n)</code> ⇒ Array(…)</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">spark<span class=\"token punctuation\">.</span>conf<span class=\"token punctuation\">.</span><span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"spark.sql.shuffle.partitions\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"5\"</span><span class=\"token punctuation\">)</span>\n\ndata<span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span><span class=\"token string\">\"count\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>take<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span>\n<span class=\"token punctuation\">[</span>Row<span class=\"token punctuation\">(</span>DEST_COUNTRY_NAME<span class=\"token operator\">=</span><span class=\"token string\">u'United States'</span><span class=\"token punctuation\">,</span> ORIGIN_COUNTRY_NAME<span class=\"token operator\">=</span><span class=\"token string\">u'Singapore'</span><span class=\"token punctuation\">,</span> count<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\nRow<span class=\"token punctuation\">(</span>DEST_COUNTRY_NAME<span class=\"token operator\">=</span><span class=\"token string\">u'Moldova'</span><span class=\"token punctuation\">,</span> ORIGIN_COUNTRY_NAME<span class=\"token operator\">=</span><span class=\"token string\">u'United States'</span><span class=\"token punctuation\">,</span> count<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n<br>\n<br>\n<h1 id=\"dataframe-and-sql\" style=\"position:relative;\"><a href=\"#dataframe-and-sql\" aria-label=\"dataframe and sql permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataFrame and SQL</h1>\n<ul>\n<li>\n<p><strong><code class=\"language-text\">createOrReplaceTempView</code></strong>: 모든 DataFrame을 table이나 view(임시 table)로 등록한다.</p>\n<p>아래의 코드 실행 하면 SQL query를 실행할 수 있게 된다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">data<span class=\"token punctuation\">.</span>createOrReplaceTempView<span class=\"token punctuation\">(</span><span class=\"token string\">\"data_table\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n</li>\n<li>SQL query는 DataFrame code와 같은 실행 계획으로 compile 되므로 둘의 성능 차이는 없다.</li>\n</ul>\n<br>\n<h3 id=\"sql\" style=\"position:relative;\"><a href=\"#sql\" aria-label=\"sql permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>sql</h3>\n<ul>\n<li><code class=\"language-text\">spark.sql</code>은 새로운 DataFrame을 return (<code class=\"language-text\">spark</code>는 <code class=\"language-text\">SparkSession</code>의 변수)</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># SQL</span>\nsqlWay <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">(</span><span class=\"token triple-quoted-string string\">\"\"\"\nSELECT DEST_COUNTRY_NAME, count(1)\nFROM data_table\nGROUP BY DEST_COUNTRY_NAME\n\"\"\"</span><span class=\"token punctuation\">)</span>\nsqlWay<span class=\"token punctuation\">.</span>explain<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span>\n<span class=\"token operator\">==</span> Physical Plan <span class=\"token operator\">==</span>\n<span class=\"token operator\">*</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span> HashAggregate<span class=\"token punctuation\">(</span>keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>DEST_COUNTRY_NAME<span class=\"token comment\">#128], functions=[count(1)])</span>\n<span class=\"token operator\">+</span><span class=\"token operator\">-</span> Exchange hashpartitioning<span class=\"token punctuation\">(</span>DEST_COUNTRY_NAME<span class=\"token comment\">#128, 5)</span>\n   <span class=\"token operator\">+</span><span class=\"token operator\">-</span> <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> HashAggregate<span class=\"token punctuation\">(</span>keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>DEST_COUNTRY_NAME<span class=\"token comment\">#128], functions=[partial_count(1)])</span>\n      <span class=\"token operator\">+</span><span class=\"token operator\">-</span> <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> FileScan csv <span class=\"token punctuation\">[</span>DEST_COUNTRY_NAME<span class=\"token comment\">#128] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string></span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># DataFrame</span>\ndataFrameWay <span class=\"token operator\">=</span> data\\\n  <span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span><span class=\"token string\">\"DEST_COUNTRY_NAME\"</span><span class=\"token punctuation\">)</span>\\\n  <span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ndataFrameWay<span class=\"token punctuation\">.</span>explain<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span>\n<span class=\"token operator\">==</span> Physical Plan <span class=\"token operator\">==</span>\n<span class=\"token operator\">*</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span> HashAggregate<span class=\"token punctuation\">(</span>keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>DEST_COUNTRY_NAME<span class=\"token comment\">#128], functions=[count(1)])</span>\n<span class=\"token operator\">+</span><span class=\"token operator\">-</span> Exchange hashpartitioning<span class=\"token punctuation\">(</span>DEST_COUNTRY_NAME<span class=\"token comment\">#128, 5)</span>\n   <span class=\"token operator\">+</span><span class=\"token operator\">-</span> <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> HashAggregate<span class=\"token punctuation\">(</span>keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>DEST_COUNTRY_NAME<span class=\"token comment\">#128], functions=[partial_count(1)])</span>\n      <span class=\"token operator\">+</span><span class=\"token operator\">-</span> <span class=\"token operator\">*</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> FileScan csv <span class=\"token punctuation\">[</span>DEST_COUNTRY_NAME<span class=\"token comment\">#128] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string></span></code></pre></div>\n<br>\n<h3 id=\"max\" style=\"position:relative;\"><a href=\"#max\" aria-label=\"max permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>max</h3>\n<ul>\n<li>DataFrame의 특정 column 값을 스캔하면서 이전 최댓값보다 더 큰 값을 찾는다.</li>\n<li>Filtering을 수행해 단일 row를 결과로 return 하는 transformation</li>\n</ul>\n<br>\n<br>\n<br>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><em>Reference</em></h3>\n<blockquote>\n<p><a href=\"https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/\"><em>Spark: The Definitive Guide</em></a>  </p>\n</blockquote>","frontmatter":{"title":"Chapter 2. A Gentle Introduction to Spark","date":"April 17, 2022","category":"[Spark: The Definitive Guide]","thumbnail":null}}},"pageContext":{"slug":"/spark-definitive-guide/gentle-introduction/","previous":{"fields":{"slug":"/spark-definitive-guide/what-is-apache-spark/"},"frontmatter":{"title":"Chapter 1. What is Apache Spark","category":"[Spark: The Definitive Guide]","draft":false}},"next":null}},"staticQueryHashes":["2353110810","3128451518"]}