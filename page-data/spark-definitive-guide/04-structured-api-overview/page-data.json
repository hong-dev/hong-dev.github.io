{"componentChunkName":"component---src-templates-blog-post-js","path":"/spark-definitive-guide/04-structured-api-overview/","result":{"data":{"site":{"siteMetadata":{"title":"hong_devlog","author":"hongdev","siteUrl":"https://hong-dev.github.io","comment":{"disqusShortName":"","utterances":"hong-dev/hong-dev.github.io"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"id":"1cb93939-97ff-5624-ab7b-29b0a794f3d7","excerpt":"Spark: The Definitive Guide 내용 정리 Overview Apache Spark Community는 2.0 version을 출시하면서 structured API를 도입했다. 1.x 버전에서는 RDD와 같은 lower-level API를 활용하는 방법이 주를 이루었으나, 2.0 출시 이후에는 자동화된 최적화 기능과 장애 대응 능력을 제공하는 structured API의 사용이 필수적이다. Structured API 종류: Dataset, DataFrame, SQL table…","html":"<blockquote>\n<p><em>Spark: The Definitive Guide 내용 정리</em></p>\n</blockquote>\n<br>\n<h1 id=\"overview\" style=\"position:relative;\"><a href=\"#overview\" aria-label=\"overview permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Overview</h1>\n<ul>\n<li>Apache Spark Community는 2.0 version을 출시하면서 structured API를 도입했다.</li>\n<li>1.x 버전에서는 RDD와 같은 lower-level API를 활용하는 방법이 주를 이루었으나, 2.0 출시 이후에는 자동화된 최적화 기능과 장애 대응 능력을 제공하는 structured API의 사용이 필수적이다.</li>\n<li>Structured API 종류: Dataset, DataFrame, SQL table/View</li>\n<li>Batch와 Streaming 처리에서 structured API 사용 가능</li>\n</ul>\n<br>\n<h1 id=\"dataframe-and-dataset\" style=\"position:relative;\"><a href=\"#dataframe-and-dataset\" aria-label=\"dataframe and dataset permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataFrame and Dataset</h1>\n<ul>\n<li>잘 정의된 row와 column을 가지는 분산 table 형태의 collection</li>\n<li>결과를 생성하기 위해 어떤 data에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획</li>\n<li>Immutable</li>\n</ul>\n<br>\n<h1 id=\"schema\" style=\"position:relative;\"><a href=\"#schema\" aria-label=\"schema permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Schema</h1>\n<ul>\n<li>분산 collection에 저장할 data type을 정의하는 방법. DataFrame의 column name과 data type을 정의</li>\n<li>Data source에서 얻거나 (schema-on-read) 직접 정의 가능</li>\n</ul>\n<br>\n<h1 id=\"structured-data-type\" style=\"position:relative;\"><a href=\"#structured-data-type\" aria-label=\"structured data type permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Structured data type</h1>\n<ul>\n<li>Spark는 사실상 programming 언어</li>\n</ul>\n<br>\n<h3 id=\"catalyst-engine\" style=\"position:relative;\"><a href=\"#catalyst-engine\" aria-label=\"catalyst engine permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Catalyst Engine</h3>\n<ul>\n<li>Spark는 실행 계획 수립과 처리에 사용하는 자체 data type 정보를 가지고 있는 catalyst engine을 사용</li>\n<li>다양한 실행 최적화 기능을 제공</li>\n</ul>\n<br>\n<h3 id=\"dataframe-vs-dataset\" style=\"position:relative;\"><a href=\"#dataframe-vs-dataset\" aria-label=\"dataframe vs dataset permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataFrame vs Dataset</h3>\n<ul>\n<li>Untyped <code class=\"language-text\">DataFrame</code> vs Typed <code class=\"language-text\">Dataset</code></li>\n<li>DataFrame에도 data type이 있지만, schema에 명시된 data type 일치 여부를 <strong>runtime</strong>이 되어서야 확인<br>\nDataset은 schema에 명시된 data type 일치 여부를 <strong>compile time</strong>에 확인</li>\n<li>Dataset은 JVM 기반의 언어인 scala와 java에서만 지원<br>\nDataset의 data type을 정의하려면 scale의 <code class=\"language-text\">case class</code>나 <code class=\"language-text\">JavaBean</code>을 사용해야 한다.</li>\n<li>DataFrame은 Row type으로 구성된 Dataset<br>\nRow type을 사용하면 garbage collection과 객체 초기화 부하가 있는 JVM data type 대신, 자체 data format을 사용하기 때문에 매우 효율적인 연산 가능<br>\n<strong>DataFrame을 사용하면 spark의 최적화된 내부 format을 사용할 수 있다.</strong><br>\n최적화된 내부 format을 사용하면, 어떤 언어 API를 사용하더라도 동일한 효과와 효율성을 얻을 수 있다.</li>\n</ul>\n<br>\n<h3 id=\"column\" style=\"position:relative;\"><a href=\"#column\" aria-label=\"column permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Column</h3>\n<ul>\n<li>Integer나 string 같은 <strong>simple type</strong>, array나 map 같은 <strong>complex type</strong>, 그리고 <strong>null value</strong>를 표현한다.</li>\n<li>Spark는 data type의 모든 정보를 추적하며 다양한 column transformation 방법을 제공</li>\n</ul>\n<br>\n<h3 id=\"row\" style=\"position:relative;\"><a href=\"#row\" aria-label=\"row permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Row</h3>\n<ul>\n<li>Data record</li>\n<li>DataFrame의 record는 Row type으로 구성된다.</li>\n<li>\n<p>Row는 SQL, RDD, Data source에서 얻거나 직접 만들 수 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">spark<span class=\"token punctuation\">.</span><span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>collect<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">[</span>Row<span class=\"token punctuation\">(</span><span class=\"token builtin\">id</span><span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> Row<span class=\"token punctuation\">(</span><span class=\"token builtin\">id</span><span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n</li>\n</ul>\n<br>\n<h3 id=\"spark-data-type\" style=\"position:relative;\"><a href=\"#spark-data-type\" aria-label=\"spark data type permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Data Type</h3>\n<ul>\n<li>Spark는 여러 가지 내부 data type을 가지고 있다.</li>\n<li>\n<p>Spark data type을 python에서 사용</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>types <span class=\"token keyword\">import</span> <span class=\"token operator\">*</span>\n\nb <span class=\"token operator\">=</span> ByteType<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nb\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> ByteType</code></pre></div>\n</li>\n</ul>\n<br>\n<h3 id=\"python-spark-data-type-mapping\" style=\"position:relative;\"><a href=\"#python-spark-data-type-mapping\" aria-label=\"python spark data type mapping permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Python-Spark data type mapping</h3>\n<table>\n<thead>\n<tr>\n<th>Spark data type</th>\n<th>Python data type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ByteType</td>\n<td>int, long (1 byte, -125~127)</td>\n</tr>\n<tr>\n<td>ShortType</td>\n<td>int, long (2 bytes, -32768~32767)</td>\n</tr>\n<tr>\n<td>IntegerType</td>\n<td>int, long (숫자값이 너무 크면 LongType 사용)</td>\n</tr>\n<tr>\n<td>LongType</td>\n<td>long (8 bytes, 더 큰 숫자는 decimal.Decimal 사용)</td>\n</tr>\n<tr>\n<td>FloatType</td>\n<td>float (4 bytes, single-precision floating point)</td>\n</tr>\n<tr>\n<td>DoubleType</td>\n<td>float</td>\n</tr>\n<tr>\n<td>DecimalType</td>\n<td>decimal.Decimal</td>\n</tr>\n<tr>\n<td>StringType</td>\n<td>string</td>\n</tr>\n<tr>\n<td>BinaryType</td>\n<td>bytearray</td>\n</tr>\n<tr>\n<td>BooleanType</td>\n<td>bool</td>\n</tr>\n<tr>\n<td>TimestampType</td>\n<td>datetime.datetime</td>\n</tr>\n<tr>\n<td>DateType</td>\n<td>datetime.date</td>\n</tr>\n<tr>\n<td>ArrayType</td>\n<td>list, tuple, array</td>\n</tr>\n<tr>\n<td>MapType</td>\n<td>dict</td>\n</tr>\n<tr>\n<td>StructType</td>\n<td>list, tuple (ex. StructType([StructField, …]))</td>\n</tr>\n<tr>\n<td>StructField</td>\n<td>각 field의 type (ex. StructField(name, dataType, [nullable]))</td>\n</tr>\n</tbody>\n</table>\n<br>\n<h1 id=\"structured-api-execution\" style=\"position:relative;\"><a href=\"#structured-api-execution\" aria-label=\"structured api execution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Structured API Execution</h1>\n<h3 id=\"execution-steps\" style=\"position:relative;\"><a href=\"#execution-steps\" aria-label=\"execution steps permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Execution steps</h3>\n<ol>\n<li>DataFrame/Dataset/SQL을 이용해 코드 작성</li>\n<li>Valid code라면, spark가 code를 <strong>Logical Plan</strong>으로 convert</li>\n<li>Spark는 Logical Plan을 <strong>Physical Plan</strong>으로 transform하며, 그 과정에서 optimizations을 할 수 있는지 확인</li>\n<li>Cluster에서 <strong>Physical Plan</strong>(RDD manipulations) 실행</li>\n</ol>\n<br>\n<h3 id=\"logical-planning\" style=\"position:relative;\"><a href=\"#logical-planning\" aria-label=\"logical planning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Logical Planning</h3>\n<ul>\n<li>Abstract transformation만 표현</li>\n<li>driver나 executor의 정보를 고려하지 않는다.</li>\n<li>사용자의 다양한 표현식을 최적화된 버전으로 변환</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">User code\n<span class=\"token operator\">-</span><span class=\"token operator\">></span> Unresolved logical plan\n   코드의 유효성과 table<span class=\"token operator\">/</span>column의 존재 여부만 판단\n   실행 계획 검증 X\n   Spark analyzer는 column<span class=\"token operator\">/</span>table을 검증하기 위해 catalog<span class=\"token punctuation\">(</span>a repository of <span class=\"token builtin\">all</span> table <span class=\"token keyword\">and</span> DF info<span class=\"token punctuation\">)</span>를 활용\n   필요한 table이나 column이 catalog에 없다면 unresolved logical plan이 만들어지지 않는다<span class=\"token punctuation\">.</span>\n<span class=\"token operator\">-</span><span class=\"token operator\">></span> Resolved logical plan\n<span class=\"token operator\">-</span><span class=\"token operator\">></span> Catalyst Optimizer로 전달 <span class=\"token punctuation\">(</span>Logical optimization<span class=\"token punctuation\">)</span>\n   Catalyst Optimizer<span class=\"token punctuation\">:</span> Predicate pushing down이나 selections를 이용해 logical plan을 optimize 하는 규칙 모음\n<span class=\"token operator\">-</span><span class=\"token operator\">></span> Optimized logical plan</code></pre></div>\n<br>\n<h3 id=\"physical-planning\" style=\"position:relative;\"><a href=\"#physical-planning\" aria-label=\"physical planning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Physical Planning</h3>\n<ul>\n<li>Spark plan 이라고도 불린다.</li>\n<li>Logical plan을 cluster 환경에서 실행하는 방법을 정의</li>\n<li>\n<p>Physical plan은 일련의 RDD와 transformation으로 변환된다.</p>\n<p>Spark는 DataFrame, Dataset, SQL로 정의된 query를 RDD transformation으로 compile 한다.</p>\n<p>따라서 spark를 <strong>compiler</strong>라고 부르기도 한다.</p>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">Optimized logical plan\n<span class=\"token operator\">-</span><span class=\"token operator\">></span> 다양한 physical plan 생성\n<span class=\"token operator\">-</span><span class=\"token operator\">></span> Cost model을 이용해서 비교\n   ex<span class=\"token punctuation\">)</span> table의 크기나 partition 수 등의 physical attributes를 고려해 연산 수행에 필요한 비용을 계산하고 비교\n<span class=\"token operator\">-</span><span class=\"token operator\">></span> Best physical plan\n<span class=\"token operator\">-</span><span class=\"token operator\">></span> Execute on the cluster</code></pre></div>\n<br>\n<h3 id=\"execution\" style=\"position:relative;\"><a href=\"#execution\" aria-label=\"execution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Execution</h3>\n<ul>\n<li>Best physical plan이 선정되면, lower-level programming interface인 RDD를 대상으로 모든 코드를 실행한다.</li>\n<li>Spark는 runtime에 전체 task나 stage를 제거할 수 있는 java byte code를 생성해 추가적인 optimization을 수행</li>\n<li>그렇게 처리한 결과를 user에게 return</li>\n</ul>\n<br>\n<br>\n<br>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><em>Reference</em></h3>\n<blockquote>\n<p><a href=\"https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/\"><em>Spark: The Definitive Guide</em></a>  </p>\n</blockquote>","frontmatter":{"title":"Chapter 4. Structured API Overview","date":"April 28, 2022","category":"[Spark: The Definitive Guide]","thumbnail":null}}},"pageContext":{"slug":"/spark-definitive-guide/04-structured-api-overview/","previous":{"fields":{"slug":"/spark-definitive-guide/03-tour-of-spark-toolset/"},"frontmatter":{"title":"Chapter 3. A Tour of Spark's Toolset","category":"[Spark: The Definitive Guide]","draft":false}},"next":null}},"staticQueryHashes":["2353110810","3128451518"]}